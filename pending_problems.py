''' 
error is happen 

Class Weights: (0.9433962264150944, 1.0638297872340425)
Random Forest Model Trained
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-55-c7e4c859d52c> in <cell line: 0>()
     91 # Test Meta Loss Function
     92 alpha = 0.01
---> 93 meta_loss_value = meta.meta_loss(meta_model, X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, alpha)
     94 print("Meta Learning Loss:", meta_loss_value.numpy())

2 frames
/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/backprop.py in _extract_tensors_and_variables(tensor)
    698       yield from _extract_tensors_and_variables(components)
    699     else:
--> 700       raise ValueError(f"Passed in object {obj} of type {type(obj).__name__!r}"
    701                        f", not tf.Tensor or tf.Variable or ExtensionType.")
    702 

ValueError: Passed in object <Variable path=dense_68/kernel, shape=(10, 100), dtype=float32, value=[[ 8.11622143e-02 -1.05842069e-01  1.77829564e-01  1.38147205e-01
   2.00608134e-01 -1.48442924e-01  1.07376695e-01  2.21103221e-01
  -1.27225459e-01  1.18186355e-01  1.44521803e-01 -1.19888224e-01
  -2.22716808e-01  1.70399666e-01 -2.12263793e-01 -8.18245560e-02
   1.75130665e-02 -1.19442374e-01 -1.16038024e-02  1.09286040e-01
   2.32555568e-02 -1.47903562e-02 -1.33132935e-01 -2.12034106e-01
  -2.00886503e-01  6.79084063e-02  1.78480625e-01 -1.27313048e-01
  -1.71254396e-01 -1.83772236e-01  7.00151026e-02 -2.11712584e-01
  -1.63856030e-01  8.50951672e-03 -6.12757951e-02 -3.24610472e-02
  -6.79362565e-02 -9.00007635e-02 -1.12871051e-01  2.10154146e-01
  -1.57158017e-01 -5.20621240e-03  9.14318860e-03 -4.81103361e-03
  -9.98573005e-02  8.78643990e-02 -1.19083375e-02 -8.35744888e-02
  -5.45468777e-02 -2.29007706e-01 -1.16763592e-01 -1.35203719e-01
  -1.03066906e-01 -4.29108441e-02  9.77075100e-02 -1.57717347e-01
   1.05673641e-01  1.88391715e-01 -9.02925879e-02 -2.97812223e-02
  -2.01719403e-01 -9.41172689e-02  2.20477819e-01  1.37083292e-01
   1.31451100e-01 -1.13101348e-01 -9.38346237e-02  8.99714231e-02
  -7.63948858e-02  1.34696066e-01  1.28339171e-01 -3.17485332e-02
   4.77509201e-02  1.76882625e-01 -6.24268055e-02  8.80789757e-02
   1.87072426e-01 -8.26188177e-02 -9.29580629e-02 -1.11732058e-01
   2.06339777e-01 -1.48143411e-01  8.70591104e-02 -4.38120663e-02
   1.73397064e-01 -5.02489060e-02  1.42434269e-01 -5.63173145e-02
   2.32190788e-02 -5.70835173e-03  1.97676837e-01 -1.87042087e-01
  -1.99922636e-01  2.26159096e-01 -5.59729189e-02 -5.24761528e-02
   1.10449344e-01  9.30757225e-02 -2.68704593e-02  3.53833735e-02]
 [-5.54514974e-02 -1.93662941e-02  8.80459845e-02 -5.18114865e-03
   2.59555578e-02  1.49614424e-01 -2.54150927e-02 -1.53512955e-02
  -1.32797778e-01 -1.87671185e-01 -2.33283967e-01 -1.01398602e-01
  -1.08158067e-01  2.11460590e-02 -2.04324350e-01 -1.57985300e-01
   6.39137924e-02 -2.12301210e-01 -2.16420993e-01 -2.75824219e-02
   1.81533366e-01  2.03857571e-01  2.31333226e-01  2.11993396e-01
  -1.27889529e-01 -1.50151491e-01  1.35336399e-01 -2.28125751e-01
   2.14301497e-01 -9.49910432e-02 -1.01953968e-01 -2.26113051e-02
   1.78947002e-01  2.19325691e-01 -1.95999026e-01 -3.90818715e-03
  -1.57571197e-01 -1.40584603e-01  8.11162293e-02 -8.92009437e-02
   8.05713534e-02 -5.46225607e-02  2.07603335e-01  8.17490518e-02
  -2.02649921e-01 -2.30935335e-01  3.67374718e-02  1.09343827e-01
   6.03349209e-02 -5.86427301e-02  1.27722651e-01 -6.01781756e-02
   1.39876604e-01  1.25622153e-02 -7.49109536e-02 -7.09006190e-03
   1.00224465e-01  4.64796424e-02  9.68225896e-02  9.11474228e-02
   6.62874877e-02 -5.82769513e-02 -1.06625572e-01  1.79390967e-01
  -1.64142281e-01  4.02191281e-02 -2.87009776e-02 -3.18228155e-02
  -2.21917808e-01 -1.61882848e-01 -1.17106155e-01  6.53430820e-04
   1.47944212e-01 -2.18903437e-01 -1.36144191e-01  1.86797738e-01
   2.22719520e-01 -1.73187375e-01 -3.73992026e-02  5.05131781e-02
   2.29668707e-01  1.70808166e-01  1.82405949e-01  1.16337448e-01
   3.16803157e-02  2.31171846e-02  1.33741558e-01  2.09286541e-01
   8.78304243e-03  1.17431521e-01  1.05886459e-01 -5.29524088e-02
   1.04886830e-01  5.56986630e-02 -5.85067421e-02 -3.25768143e-02
   2.62670219e-02  3.49557400e-02 -1.84279397e-01 -1.02570653e-02]
 [-4.02346253e-02 -4.32651937e-02  1.48658574e-01  1.42919719e-01
  -8.35572332e-02 -1.58891082e-01  9.07294154e-02 -1.57214150e-01
  -1.35241747e-01 -1.94218740e-01  2.07802325e-01  2.69974768e-02
   1.00607336e-01  3.99178863e-02  1.29861534e-01  1.55340254e-01
  -8.71442407e-02  8.89123976e-02  1.29658014e-01 -1.98150992e-01
   8.23274851e-02 -4.94352281e-02 -1.09608047e-01  1.23147339e-01
  -1.32706299e-01 -2.00241253e-01  7.73788691e-02  2.16662437e-01
   1.82563066e-02  1.81478828e-01  1.04192972e-01 -1.03040293e-01
   2.25806832e-02 -1.69132620e-01  8.40291381e-02 -1.21860944e-01
  -1.35440424e-01 -6.75646365e-02 -3.31334770e-02  6.84608519e-02
  -1.81977257e-01  9.03121829e-02 -6.96164817e-02 -2.23267555e-01
  -5.59420884e-03 -2.00872466e-01 -1.64459735e-01 -1.92210212e-01
  -1.63366586e-01  6.76746070e-02  6.77151382e-02  9.25272405e-02
  -9.60416049e-02 -2.01139808e-01 -1.41965985e-01 -7.21093863e-02
   1.15298748e-01  1.47147566e-01  2.08548158e-01  1.61206752e-01
  -1.42143831e-01 -1.51081607e-01  2.00927258e-02  1.80805594e-01
  -2.22836584e-01 -4.61242199e-02  5.85287511e-02  1.25029564e-01
   6.59838021e-02  2.56651640e-02 -3.82397324e-02  1.56064808e-01
   7.92296231e-03  1.51206166e-01  2.32124925e-01  2.18350053e-01
  -2.39312053e-02 -2.13753462e-01  6.76976740e-02 -1.35879755e-01
  -2.02227890e-01 -1.89731106e-01 -9.30306762e-02  1.75378710e-01
   1.68457955e-01  9.66094434e-02 -4.16216254e-03 -3.08372974e-02
  -2.30154321e-01 -1.19767226e-01 -7.18691796e-02 -1.20177269e-01
   1.65854394e-02  2.84525156e-02  6.44902289e-02  1.20914161e-01
   2.30994731e-01  3.38392556e-02 -1.16840325e-01  1.70312703e-01]
 [ 5.34197688e-02  6.59193695e-02 -1.85983568e-01  2.05204636e-01
   6.94164038e-02 -4.60333973e-02 -2.22468674e-02  8.97613764e-02
  -5.26389480e-03  1.92442238e-02 -1.61732525e-01 -1.77398026e-01
  -1.29862249e-01 -1.86434478e-01  2.31963009e-01 -1.35440648e-01
  -8.35740417e-02  1.02242231e-01  1.63189679e-01  2.24429995e-01
   6.68806732e-02 -1.74662560e-01 -1.83610708e-01 -1.93007201e-01
  -2.20259860e-01  3.14075947e-02  8.91261697e-02 -4.48637903e-02
   3.35715711e-03 -1.25429928e-01 -1.06351793e-01  9.77341235e-02
  -2.61932611e-04  2.11485952e-01 -3.25064957e-02  2.13990718e-01
  -3.87375355e-02 -1.30577609e-01 -1.69091806e-01  1.43218338e-01
   3.82426381e-02  1.78734928e-01 -1.87177062e-02 -7.01030344e-02
  -2.20829949e-01 -4.38306034e-02  7.34083056e-02 -1.13227747e-01
   2.21870989e-01  9.36349928e-02  1.45742923e-01  2.01025307e-01
   1.11454874e-02  2.32770532e-01 -9.21380222e-02  4.25353646e-04
   3.98612022e-02  7.69644082e-02  8.71288776e-02 -1.75871253e-02
  -6.62956238e-02  1.77216440e-01  3.33775282e-02 -1.78371966e-01
  -2.01276615e-01  1.43185318e-01 -2.16933787e-01  6.81489706e-02
   4.70575094e-02  1.88050330e-01 -1.66314423e-01 -2.57974118e-02
  -1.49653852e-01  2.07017332e-01  1.45358056e-01  2.04438120e-01
  -2.19661608e-01 -3.17708105e-02 -5.25689274e-02 -6.43565953e-02
  -1.17437743e-01 -2.30120867e-01 -6.90972954e-02  1.63076967e-02
   4.96199727e-02 -2.20729709e-02  6.97679520e-02  5.87070882e-02
   3.03656161e-02 -5.07261157e-02  9.00712609e-02  4.85912859e-02
  -1.18794449e-01  2.29607195e-01 -1.76788583e-01  2.06148177e-01
   1.58863455e-01  2.61526704e-02 -1.48020297e-01 -1.11493237e-01]
 [-5.06964922e-02 -1.81485072e-01  2.28049964e-01 -1.97386578e-01
  -1.53582588e-01  4.22443748e-02 -1.87267154e-01 -2.11621493e-01
   7.12911487e-02  6.44441843e-02 -9.94842798e-02 -3.84659767e-02
   9.51442719e-02 -2.02706382e-01 -2.79125571e-03 -1.89552486e-01
  -7.97646940e-02 -1.45042002e-01 -4.99116480e-02 -1.51333243e-01
  -1.46685809e-01  1.12471014e-01 -2.65607089e-02 -1.51612207e-01
   1.07570440e-02  1.05267376e-01  1.79568857e-01  2.31413364e-01
   8.57609808e-02  1.38673186e-01 -2.15379506e-01  8.28179419e-02
   5.39472997e-02 -9.29586142e-02 -1.71522230e-01  1.47812545e-01
   3.54299247e-02 -3.08346152e-02 -1.33615151e-01  6.52794242e-02
   4.43671644e-02 -6.52800947e-02  5.97107708e-02  2.16023266e-01
   1.06246322e-01 -1.55738950e-01  3.73181999e-02 -1.44587457e-01
   6.43947423e-02 -2.06758514e-01 -2.30837658e-01  2.09362209e-02
  -2.09280655e-01  1.77629232e-01  7.78180361e-03 -1.49086952e-01
   5.18666506e-02 -1.51895583e-01  1.92046881e-01 -8.06038082e-03
   2.12796390e-01  5.73031008e-02 -1.08428299e-01 -2.02979222e-01
  -7.20321089e-02 -7.82916099e-02  2.25979686e-01 -4.50418591e-02
   1.42855346e-01  2.22478151e-01 -4.02580649e-02 -1.09184027e-01
   1.40983343e-01 -5.05341738e-02 -7.70512223e-02  8.90107751e-02
  -1.50253892e-01  1.92125797e-01  1.76650763e-01  2.16557294e-01
   2.01179177e-01 -1.95354611e-01  1.18734479e-01 -1.83431298e-01
  -1.29956365e-01 -5.49470186e-02  2.29688555e-01 -2.55925506e-02
  -5.73925376e-02 -1.57534942e-01  2.59876847e-02  1.33823246e-01
   2.29220092e-01 -1.25693411e-01  1.81056529e-01  1.58896714e-01
   4.67045605e-02 -1.66442707e-01 -1.90000832e-01 -2.13048354e-01]
 [-1.54277563e-01 -1.95591643e-01  1.07186258e-01  8.08652639e-02
  -1.88769698e-01  4.74250913e-02 -1.00319639e-01  7.57973492e-02
  -1.28693372e-01 -9.91131663e-02 -7.05562383e-02 -1.88221052e-01
   3.11640799e-02 -2.19720364e-01  4.75975275e-02  1.05717331e-01
   1.63136333e-01 -1.59708858e-02  8.43338966e-02 -2.14061111e-01
   1.59737527e-01  4.29129601e-02  3.46580148e-03  4.66118753e-03
  -8.74227136e-02 -1.20161794e-01 -9.89074111e-02  1.93529487e-01
   8.53919983e-03  8.76967311e-02 -9.65012610e-02 -6.96115792e-02
  -1.91510499e-01  3.34776044e-02  2.25390911e-01  1.72470570e-01
  -1.34867728e-02  2.11319596e-01  1.71244055e-01 -4.67948020e-02
   1.41922832e-01 -2.40610540e-03 -1.69590712e-02  1.22422516e-01
  -2.10351825e-01  1.26538396e-01 -1.78080022e-01 -2.25075856e-01
   1.06976390e-01  5.66573143e-02  1.71884745e-01  1.04877919e-01
  -1.92996785e-01  1.13006532e-01 -6.57176971e-02 -2.07981691e-01
  -2.10663676e-03 -2.32881218e-01 -3.30907106e-02  1.76272094e-02
   1.26277119e-01  9.27179754e-02 -1.36310637e-01  1.31867379e-01
   1.23116136e-01  1.16043836e-01  5.11090457e-02  1.64605469e-01
  -9.50699896e-02 -6.33732378e-02 -1.07650757e-01 -1.93165004e-01
  -1.35175705e-01 -2.01865956e-01  2.04310328e-01  8.75318944e-02
   1.42746419e-01 -2.12578341e-01  1.66789711e-01 -1.00640535e-01
  -8.98813754e-02  2.08953410e-01  2.08593696e-01 -8.92454833e-02
  -1.95313007e-01  1.01547092e-01 -9.57712531e-02  1.09238625e-02
   2.22220778e-01 -6.23136014e-02 -1.96318358e-01  2.44168639e-02
   1.17946029e-01  2.24933028e-01  1.82919502e-02 -8.43051076e-03
  -6.82617277e-02 -6.27623498e-02 -1.63078874e-01 -1.29692763e-01]
 [-1.65005654e-01 -2.03973487e-01 -8.44284445e-02  7.31822252e-02
  -2.24236608e-01  9.42041278e-02 -1.52432129e-01 -1.92701384e-01
  -1.21147096e-01  1.04723454e-01  1.90586001e-01 -4.95664626e-02
  -1.17157213e-01  3.06319296e-02  1.81862980e-01  1.33918226e-02
  -3.87288034e-02  1.98679566e-01  1.07497633e-01 -1.85963854e-01
   1.80187941e-01  2.08382279e-01  1.86322123e-01 -1.01520121e-03
  -6.54459745e-02  3.97485197e-02 -3.02055478e-03 -4.09679562e-02
   2.02633649e-01 -1.92398310e-01 -4.79618013e-02  1.89468354e-01
   3.71686816e-02 -1.45376086e-01 -5.77472448e-02  2.32237369e-01
   3.09047699e-02 -6.26325011e-02  1.08070165e-01  1.67370260e-01
  -7.57324845e-02 -1.88572407e-02  9.35314894e-02 -6.14035726e-02
  -2.55416334e-03 -1.33826733e-01  1.12393111e-01 -1.27212986e-01
  -9.68626440e-02 -1.43537506e-01 -1.03361309e-02  4.65109944e-02
   1.08819067e-01  1.50724053e-01  2.16425359e-02  6.44302070e-02
  -3.12108696e-02  1.77058041e-01  3.82471979e-02  1.32273972e-01
   1.08083069e-01  1.06992185e-01 -1.32984713e-01  2.23304987e-01
  -1.93740875e-01 -6.33479059e-02 -1.35356516e-01 -4.26626056e-02
  -3.53017598e-02 -1.29394740e-01 -7.11027682e-02 -2.05216050e-01
   7.77408481e-02 -2.07573593e-01  1.63376331e-04 -9.23826396e-02
   1.40248328e-01 -5.49791455e-02 -2.07532614e-01  1.47903025e-01
  -1.64101973e-01  1.94912761e-01 -1.80451721e-02  2.08475918e-01
  -7.05928206e-02  2.15399116e-01 -2.25812361e-01 -6.38072193e-03
  -8.78153294e-02  2.97258794e-02  2.07294554e-01 -1.22322500e-01
  -1.05777591e-01 -1.47892326e-01 -2.04589903e-01 -3.94724309e-02
   1.33103132e-03  2.59017646e-02  2.35633552e-02 -1.62613183e-01]
 [ 3.06207538e-02 -6.53906167e-02  1.59086823e-01 -1.84936896e-01
  -2.41149962e-03  3.14230323e-02 -1.41618967e-01 -2.19952330e-01
   1.38727814e-01 -2.08717883e-02  5.85751832e-02 -9.92815495e-02
   1.45093799e-02  7.90785253e-02  1.91267341e-01  9.04074311e-03
   3.59583795e-02 -1.72926098e-01 -5.68362176e-02 -1.75351411e-01
   1.49056435e-01  3.20508480e-02 -7.40202516e-02 -1.13850452e-01
   1.88451916e-01  1.58795357e-01  5.32128513e-02  1.14895433e-01
   1.04628086e-01  9.00482535e-02  1.21967137e-01  2.50271559e-02
  -6.97866529e-02  1.05664343e-01  8.60507786e-03 -1.32197797e-01
   7.34014511e-02 -1.69596076e-01 -1.05177164e-01 -2.61228234e-02
   2.64645815e-02  1.13922119e-01  1.08108073e-01  1.02701247e-01
   2.19893873e-01 -4.60378528e-02 -1.92157924e-01  1.92057014e-01
  -9.24451202e-02  4.15256321e-02  2.18461156e-01 -8.31087679e-02
   4.65878844e-02 -1.73603594e-01 -9.94864553e-02  2.29678839e-01
   1.87444657e-01 -8.97215754e-02  1.02724344e-01  1.96624160e-01
  -1.26572028e-01 -2.20103234e-01 -2.17185184e-01  1.06869251e-01
  -1.70243770e-01  1.85198784e-01 -4.76683527e-02  2.46731639e-02
  -2.70551592e-02 -7.80829638e-02  1.60869211e-02  3.48244011e-02
   4.89503145e-04  1.45510495e-01  1.44614518e-01  1.37467951e-01
   9.08558667e-02  7.63231218e-02 -6.51495159e-02  4.36699092e-02
   2.18946636e-01 -2.31691450e-01  3.76894176e-02 -4.34035212e-02
   1.82333738e-01  1.63466126e-01  1.48016274e-01 -9.71800238e-02
  -1.91827223e-01 -2.12218583e-01 -6.06662035e-03 -2.21372902e-01
  -1.16893351e-02 -1.52412593e-01  1.25543624e-01 -1.71706498e-01
  -1.13242269e-02 -7.54836500e-02 -8.63520503e-02  7.41918683e-02]
 [-1.37258619e-02 -1.55795425e-01 -3.10760587e-02  1.76558346e-01
   8.89173150e-03 -1.13875061e-01 -1.63066566e-01  1.59249902e-02
  -3.13281864e-02  2.01567680e-01  3.61989141e-02  1.45745486e-01
   6.48553967e-02 -7.53590316e-02 -1.18362516e-01 -1.13212720e-01
  -2.28270814e-01  6.08273745e-02 -5.21706343e-02 -1.34543046e-01
   7.17267394e-02 -3.28733325e-02 -1.23968199e-01 -1.67437911e-01
  -5.06328344e-02 -2.82093883e-03  1.29884928e-01 -2.26009488e-02
  -2.18790457e-01  1.02072239e-01 -1.93967044e-01  1.35972589e-01
  -1.54904217e-01 -5.56588620e-02  7.31981099e-02 -5.74193746e-02
  -1.08930282e-01 -6.39569610e-02  2.04431921e-01  1.53000325e-01
  -6.73444122e-02  1.85592324e-01 -2.09483385e-01  5.06890714e-02
  -1.88341290e-02  1.98722959e-01  1.45227909e-01  1.69312626e-01
  -1.62312061e-01  5.31993508e-02  8.06430876e-02 -2.12752789e-01
   2.13529915e-01  3.55561674e-02 -1.81724623e-01  5.26069701e-02
   3.43742967e-02  1.35564655e-01 -2.27557793e-01 -4.45094854e-02
   7.31695890e-02 -1.80467978e-01 -1.14239335e-01  1.16405040e-02
   7.71671534e-03  1.58282131e-01  7.07932711e-02  1.97440356e-01
  -5.91446459e-02 -1.72933072e-01  2.13013887e-01 -7.40877986e-02
   1.46649897e-01  1.40338987e-02  1.18856132e-02  2.24791974e-01
   2.01735318e-01  1.13121718e-01 -2.03540787e-01  9.68870819e-02
  -2.11809471e-01  1.96294188e-01  1.21084452e-01 -7.64270723e-02
  -1.03372321e-01 -1.34492993e-01  2.60038674e-02 -1.98901758e-01
   1.88254029e-01  1.15213990e-01 -6.32198155e-03 -7.10666180e-03
  -2.04705656e-01  2.52536535e-02 -1.71710387e-01  1.72820985e-01
   1.54086351e-01  2.24508226e-01 -3.77579182e-02  4.36360538e-02]
 [ 2.32576728e-01 -2.12681800e-01 -1.14416018e-01  7.37339258e-02
   1.86236858e-01 -8.23492557e-02  3.59280109e-02 -2.12448984e-01
   1.34176165e-02 -7.29072690e-02 -2.41862237e-03  8.48264694e-02
   1.81751013e-01 -1.61965430e-01  4.03304994e-02  1.96036696e-03
   8.20719600e-02 -2.17042521e-01  1.44632339e-01  1.73085749e-01
  -1.77617535e-01  4.06996310e-02  4.13128436e-02 -7.51242191e-02
  -1.72316328e-01 -1.99555740e-01 -1.36274427e-01  1.26343071e-01
   2.21166760e-01  1.18303746e-02  1.23934895e-01 -5.60237020e-02
  -8.34832340e-02  1.90727711e-01  8.98769796e-02  4.21277285e-02
  -1.98261350e-01 -4.60590720e-02 -2.29479343e-01 -1.78361341e-01
   4.04069126e-02  1.67691827e-01  2.32726574e-01 -2.47829854e-02
   2.16781825e-01  1.41714573e-01  1.72839284e-01 -1.84918582e-01
  -1.35811597e-01 -5.18000573e-02 -1.78899959e-01 -1.64400041e-01
   7.43276775e-02 -1.91714242e-01  2.10721374e-01  2.07393110e-01
   9.13329124e-02  2.26624042e-01  8.76405537e-02 -1.01600334e-01
  -1.95856750e-01  6.47075474e-03 -1.90533221e-01  2.22530812e-01
  -1.27158418e-01 -3.29468250e-02  1.41429096e-01 -7.37757981e-02
  -1.59465969e-02 -7.43893683e-02  1.17771834e-01  1.77801847e-01
  -1.24944478e-01  1.82723224e-01 -1.08385041e-01 -1.61132097e-01
  -1.91421911e-01  1.24322772e-01 -4.05225009e-02 -1.07414216e-01
   1.80781424e-01  1.37783617e-01  5.34683168e-02  1.19951874e-01
   9.67227221e-02  2.02809900e-01 -1.56632811e-02  5.16017675e-02
  -1.00449383e-01 -1.02390587e-01 -6.45111203e-02  2.13035673e-01
   5.17525077e-02 -1.89163372e-01 -1.87201455e-01 -1.01359636e-02
  -2.02027559e-01  3.24395299e-03  2.97922492e-02  1.62003577e-01]]> of type 'Variable', not tf.Tensor or tf.Variable or ExtensionType.
''' 



class extremly_imblanced: 

    def __init__(self, x , y ): 
        self.x = x 
        self.y = y 
    
    def imblance_handing(self): 
        from sklearn.utils.class_weight import compute_class_weight
        import numpy as np 
        class_weight = compute_class_weight('balanced', classes=np.unique(self.y), y=self.y)
        class_0, class_1 = class_weight[0], class_weight[1]
        return class_0, class_1 

    def random_forest(self, n_estimators=200): 
        from sklearn.ensemble import RandomForestClassifier 
        _0, _1 = self.imblance_handing() 
        rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=4, class_weight={0: _0, 1: _1}) 
        return rf
    
    def rf_call(self):
        model = self.random_forest()
        return model 
  
    class meta_learning: 
        import tensorflow as tf 

        def __init__(self, input_shape): 
            self.input_shape = input_shape 

        def model(self): 
            input = self.tf.keras.layers.Input(shape=(self.input_shape,))
            x = self.tf.keras.layers.Dense(100, activation='relu')(input)
            x = self.tf.keras.layers.Dense(200, activation='relu')(x)
            output = self.tf.keras.layers.Dense(1, activation='sigmoid')(x)
            model = self.tf.keras.models.Model(inputs=input, outputs=output)
            return model 
        
        def loss(self, x, y_true, model): 
            y_predict = model(x, training=True)
            pr_auc = self.tf.keras.metrics.AUC(curve='PR')
            pr_auc.update_state(y_true, y_predict)
            return pr_auc.result()  # Return Tensor, not numpy
        
        def meta_loss(self, model, x_train, x_test, y_train, y_test, alpha):
            with self.tf.GradientTape() as tape: 
                tape.watch(model.trainable_variables)
                loss = self.loss(x_train, y_train, model)
            gradients = tape.gradient(loss, model.trainable_variables)
            
            model_temp = self.model()
            model_temp.set_weights(model.get_weights())  # Correct method name
            
            # Convert gradients to numpy arrays for weight update
            updated_weights = [w - (alpha * g) for w, g in zip(model.trainable_variables, gradients)]
            model_temp.set_weights(updated_weights)  # Use numpy arrays
            
            meta_loss = self.loss(x_test, y_test, model_temp)
            return meta_loss

# Sample Data
import numpy as np
import tensorflow as tf

X_train = np.random.rand(100, 10)
y_train = np.random.randint(0, 2, 100)
X_test = np.random.rand(20, 10)
y_test = np.random.randint(0, 2, 20)

# Instantiate and test the class
imb_model = extremly_imblanced(X_train, y_train)

# Test imbalance handling
print("Class Weights:", imb_model.imblance_handing())

# Test Random Forest model
rf_model = imb_model.rf_call()
rf_model.fit(X_train, y_train)
print("Random Forest Model Trained")

# Test Meta Learning Model
meta = imb_model.meta_learning(10)
meta_model = meta.model()
meta_model.compile(optimizer='adam', loss='binary_crossentropy')

# Generate random tensor data for testing
X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)
y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)
X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)
y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)

# Test Meta Loss Function
alpha = 0.01
meta_loss_value = meta.meta_loss(meta_model, X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, alpha)
print("Meta Learning Loss:", meta_loss_value.numpy())



